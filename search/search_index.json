{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"K-Nearest Neighbors Pendahuluan Pengertian KNN Algoritma KNN adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. Data pembelajaran diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. \u200b Nilai k yang terbaik untuk algoritma ini tergantung pada data. Secara umum, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritma k-nearest neighbor. \u200b Ketepatan algoritma k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritma ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur agar performa klasifikasi menjadi lebih baik. Sesuai dengan prinsip kerja K-Nearest Neighbor yaitu mencari jarak terdekat antara data yang akan dievaluasi dengan k tetangga(neighbor) terdekatnya dalam data pelatihan. Persamaan dibawah ini menunjukkan rumus perhitungan untuk mencari jarak terdekat dengan d adalah jarak dan p adalah dimensi data(Agusta, 2007): Dengan keterangan : \ud835\udc651 : sampel data \ud835\udc652 : data uji i : data ke-i d : jarak euclidean p : dimensi data Algoritma \u200b Algoritma metode KNN sangatlah sederhana, bekerja berdasarkan jarak terpendek dari query instance ke training sample untuk menentukan KNN-nya. Training sample diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi training sample. Sebuah titik pada ruang ini ditandai kelac c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat dari titik tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan Euclidean Distance. Tentukan parameter K (jumlah tetangga paling dekat). Hitung kuadrat jarak euclid masing-masing objek terhadap data sample yang diberikan. Urutkan objek-objek kedalam kelompok yang memiliki jarak terkecil. Kumpulkan kategori Y (klasifikasi nearest neighbor). Dengan kategori nearest neighbor yang paling banyak, maka dapt diprediksikan nilai query instance yang telah dihitung. Kelebihan Mudah dipahami dan diimplementasikan. Sangan non-linear . K-NN merupakan salah satu algoritma (model) pembelajaran mesin yang bersifat non-parametrik , yaitu model yang tidak mengasumsikan apa-apa mengenai distribusi instance di dalam dataset . Model non-parametrik biasanya lebih sulit diinterpretasikan, namun salah satu kelebihannya adalah garis keputusan kelas yang dihasilkan model tersebut bisa jadi sangat fleksibel dan non-linear . Kuat dalam hal ruang pencarian, misalnya kelas tidak harus linear dipisahkan. Efektif apabila data training sample -nya besar. Tangguh terhadap data training sample -nya besar. Beberapa parameter untuk acuan: jarak metrik dan k. Memiliki konsistensi yang kuat. Kekurangan Perlu untuk menentukan nilai k yang optimal sehingga untuk menyatakan jumlah tetangga terdekatnya lebih mudah. Nilai komputasi yang cukup tinggi karena perhitungan jarak harus dilkukan pada setiap query instance. Tidak menangani missing value secara implisit. Implementasi (Studi Kasus) Alat & Bahan Sebelum menerapkan konsep k-nearest neighbor pada studi kasus yang telah ditentukan, beberapa tools yang perlu dipersiapkan agar program yang kita rancang bisa dieksekusi dengan baik diantaranya: python 3.x (versi 3 keatas). Anaconda Navigator atau Pycharm. Untuk mempermudah kawan-kawan mendapatkan toolsnya, sillakan kawan-kawan bisa download tools-nya disini . Studi kasus pada Fruit Data adalah mengklasifikasikan beberapa jenis buah dengan 3 ketentuan inputan. Untuk contoh penulisan program dan datanya bisa mengambil kawan-kawan download disini . Berikut source code dan penjelasan untuk menyelesaikan study kasus tersebut dengan K-NN Classification. Langkah-langkah: Pertama Import beberapa library dari python seperti: pandas => memuat sebuah file ke dalam tabel virtual ala spreadsheet . numpy => untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. matpolib => untuk menyajikan visualisasi data cluster . sklearn => untuk mengimportkan library data science #import library import numpy as np import matplotlib.pyplot as plt import pandas as pd from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn import neighbors from matplotlib.colors import ListedColormap, BoundaryNorm import matplotlib.patches as mpatches Kedua Mengimport data dari komputer dengan perintah pandas. #read data data = pd.read_table('fruit_data.txt') #Pastikan file data set berada dalam folder yang sama dengan file jupyter notebook Ketiga Menampilkan data. #explore data print(data.shape) data.head(10) #Menampilkan 10 baris pertama dari tabel # membuat nilai kunci utama antara fruit_label dengan fruit_name lookup_fruit_name = dict(zip(data.fruit_label.unique(), data.fruit_name.unique())) print(lookup_fruit_name) Keempat Membuat model dan data train. X = data[['mass', 'width', 'height']] y = data['fruit_label'] Kelima Melakukan split (memisah) data antara data test dan data train. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0) Keenam Mengecek nilai dengan dimensi array. print('X_train = ', X_train.shape) print('X_test = ', X_test.shape) print('y_train = ', y_train.shape) print('y_test = ', y_test.shape) X_train.head() y_train.head() Ketujuh Menentukan objek kelas knn. knn = KNeighborsClassifier(n_neighbors = 5) Kedelapan Memasukkan nilai data train kedalam fungsi knn. knn.fit(X_train, y_train) Kesembilan Mengecek nilai akurasi dari data test. knn.score(X_test, y_test) Kesepuluh Melakukan ploting data. Mengklasifikasikan data berdasarkan jarak data dengan data tetangga terdekat menggunakan warna plot agar mempermudah membaca data. def plot_fruit_knn(X, y, n_neighbors, weights): X_mat = X[['height', 'width']].as_matrix() y_mat = y.as_matrix() # Create color maps cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF','#AFAFAF']) cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF','#AFAFAF']) clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights) clf.fit(X_mat, y_mat) # Plot the decision boundary by assigning a color in the color map # to each mesh point. mesh_step_size = .01 # step size in the mesh plot_symbol_size = 50 x_min, x_max = X_mat[:, 0].min() - 1, X_mat[:, 0].max() + 1 y_min, y_max = X_mat[:, 1].min() - 1, X_mat[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_step_size), np.arange(y_min, y_max, mesh_step_size)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) # Put the result into a color plot Z = Z.reshape(xx.shape) plt.figure() plt.pcolormesh(xx, yy, Z, cmap=cmap_light) # Plot training points plt.scatter(X_mat[:, 0], X_mat[:, 1], s=plot_symbol_size, c=y, cmap=cmap_bold, edgecolor = 'black') plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) patch0 = mpatches.Patch(color='#FF0000', label='apple') patch1 = mpatches.Patch(color='#00FF00', label='mandarin') patch2 = mpatches.Patch(color='#0000FF', label='orange') patch3 = mpatches.Patch(color='#AFAFAF', label='lemon') plt.legend(handles=[patch0, patch1, patch2, patch3]) plt.xlabel('height (cm)') plt.ylabel('width (cm)') plt.show() plot_fruit_knn(X_train, y_train, 5, 'uniform') # n_neighbors = 5 Kesebelas Melakukan prediksi terhadap beberapa data baru. fruit_prediction = knn.predict([[30, 6, 5]]) lookup_fruit_name[fruit_prediction[0]] Output: mandarin fruit_prediction = knn.predict([[500, 500, 500]]) lookup_fruit_name[fruit_prediction[0]] Output: orange Selanjutnya ulangi langkah kesebelas untuk melakukan test data baru untuk diuji dengan K-Nearest Neighbor Classification. Refrensi: Gorunescu, F. 2011. Data Mining Concept Model and Techniques. Berlin: Springer. ISBN 978-3-642-19720-8. Han, Jiawei dan Kamber, Micheline. (2006), Data Mining : Concept and Techniques Second Edition, Morgan Kaufmann Publishers. Florin Gorunescu, Data Mining: Concepts, Models and Techniques, Springer, 2011. https://cahyadsn.phpindonesia.id/extra/knn.php https://www.academia.edu/31306621/MAKALAH_KNN_K-NEAREST_NEIGHBOUR_","title":"K-NN"},{"location":"#k-nearest-neighbors","text":"","title":"K-Nearest Neighbors"},{"location":"#pendahuluan","text":"","title":"Pendahuluan"},{"location":"#pengertian-knn","text":"Algoritma KNN adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. Data pembelajaran diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. \u200b Nilai k yang terbaik untuk algoritma ini tergantung pada data. Secara umum, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritma k-nearest neighbor. \u200b Ketepatan algoritma k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritma ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur agar performa klasifikasi menjadi lebih baik. Sesuai dengan prinsip kerja K-Nearest Neighbor yaitu mencari jarak terdekat antara data yang akan dievaluasi dengan k tetangga(neighbor) terdekatnya dalam data pelatihan. Persamaan dibawah ini menunjukkan rumus perhitungan untuk mencari jarak terdekat dengan d adalah jarak dan p adalah dimensi data(Agusta, 2007): Dengan keterangan : \ud835\udc651 : sampel data \ud835\udc652 : data uji i : data ke-i d : jarak euclidean p : dimensi data","title":"Pengertian KNN"},{"location":"#algoritma","text":"\u200b Algoritma metode KNN sangatlah sederhana, bekerja berdasarkan jarak terpendek dari query instance ke training sample untuk menentukan KNN-nya. Training sample diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi training sample. Sebuah titik pada ruang ini ditandai kelac c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat dari titik tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan Euclidean Distance. Tentukan parameter K (jumlah tetangga paling dekat). Hitung kuadrat jarak euclid masing-masing objek terhadap data sample yang diberikan. Urutkan objek-objek kedalam kelompok yang memiliki jarak terkecil. Kumpulkan kategori Y (klasifikasi nearest neighbor). Dengan kategori nearest neighbor yang paling banyak, maka dapt diprediksikan nilai query instance yang telah dihitung.","title":"Algoritma"},{"location":"#kelebihan","text":"Mudah dipahami dan diimplementasikan. Sangan non-linear . K-NN merupakan salah satu algoritma (model) pembelajaran mesin yang bersifat non-parametrik , yaitu model yang tidak mengasumsikan apa-apa mengenai distribusi instance di dalam dataset . Model non-parametrik biasanya lebih sulit diinterpretasikan, namun salah satu kelebihannya adalah garis keputusan kelas yang dihasilkan model tersebut bisa jadi sangat fleksibel dan non-linear . Kuat dalam hal ruang pencarian, misalnya kelas tidak harus linear dipisahkan. Efektif apabila data training sample -nya besar. Tangguh terhadap data training sample -nya besar. Beberapa parameter untuk acuan: jarak metrik dan k. Memiliki konsistensi yang kuat.","title":"Kelebihan"},{"location":"#kekurangan","text":"Perlu untuk menentukan nilai k yang optimal sehingga untuk menyatakan jumlah tetangga terdekatnya lebih mudah. Nilai komputasi yang cukup tinggi karena perhitungan jarak harus dilkukan pada setiap query instance. Tidak menangani missing value secara implisit.","title":"Kekurangan"},{"location":"#implementasi-studi-kasus","text":"","title":"Implementasi (Studi Kasus)"},{"location":"#alat-bahan","text":"Sebelum menerapkan konsep k-nearest neighbor pada studi kasus yang telah ditentukan, beberapa tools yang perlu dipersiapkan agar program yang kita rancang bisa dieksekusi dengan baik diantaranya: python 3.x (versi 3 keatas). Anaconda Navigator atau Pycharm. Untuk mempermudah kawan-kawan mendapatkan toolsnya, sillakan kawan-kawan bisa download tools-nya disini . Studi kasus pada Fruit Data adalah mengklasifikasikan beberapa jenis buah dengan 3 ketentuan inputan. Untuk contoh penulisan program dan datanya bisa mengambil kawan-kawan download disini . Berikut source code dan penjelasan untuk menyelesaikan study kasus tersebut dengan K-NN Classification.","title":"Alat &amp; Bahan"},{"location":"#langkah-langkah","text":"","title":"Langkah-langkah:"},{"location":"#pertama","text":"Import beberapa library dari python seperti: pandas => memuat sebuah file ke dalam tabel virtual ala spreadsheet . numpy => untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. matpolib => untuk menyajikan visualisasi data cluster . sklearn => untuk mengimportkan library data science #import library import numpy as np import matplotlib.pyplot as plt import pandas as pd from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn import neighbors from matplotlib.colors import ListedColormap, BoundaryNorm import matplotlib.patches as mpatches","title":"Pertama"},{"location":"#kedua","text":"Mengimport data dari komputer dengan perintah pandas. #read data data = pd.read_table('fruit_data.txt') #Pastikan file data set berada dalam folder yang sama dengan file jupyter notebook","title":"Kedua"},{"location":"#ketiga","text":"Menampilkan data. #explore data print(data.shape) data.head(10) #Menampilkan 10 baris pertama dari tabel # membuat nilai kunci utama antara fruit_label dengan fruit_name lookup_fruit_name = dict(zip(data.fruit_label.unique(), data.fruit_name.unique())) print(lookup_fruit_name)","title":"Ketiga"},{"location":"#keempat","text":"Membuat model dan data train. X = data[['mass', 'width', 'height']] y = data['fruit_label']","title":"Keempat"},{"location":"#kelima","text":"Melakukan split (memisah) data antara data test dan data train. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)","title":"Kelima"},{"location":"#keenam","text":"Mengecek nilai dengan dimensi array. print('X_train = ', X_train.shape) print('X_test = ', X_test.shape) print('y_train = ', y_train.shape) print('y_test = ', y_test.shape) X_train.head() y_train.head()","title":"Keenam"},{"location":"#ketujuh","text":"Menentukan objek kelas knn. knn = KNeighborsClassifier(n_neighbors = 5)","title":"Ketujuh"},{"location":"#kedelapan","text":"Memasukkan nilai data train kedalam fungsi knn. knn.fit(X_train, y_train)","title":"Kedelapan"},{"location":"#kesembilan","text":"Mengecek nilai akurasi dari data test. knn.score(X_test, y_test)","title":"Kesembilan"},{"location":"#kesepuluh","text":"Melakukan ploting data. Mengklasifikasikan data berdasarkan jarak data dengan data tetangga terdekat menggunakan warna plot agar mempermudah membaca data. def plot_fruit_knn(X, y, n_neighbors, weights): X_mat = X[['height', 'width']].as_matrix() y_mat = y.as_matrix() # Create color maps cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF','#AFAFAF']) cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF','#AFAFAF']) clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights) clf.fit(X_mat, y_mat) # Plot the decision boundary by assigning a color in the color map # to each mesh point. mesh_step_size = .01 # step size in the mesh plot_symbol_size = 50 x_min, x_max = X_mat[:, 0].min() - 1, X_mat[:, 0].max() + 1 y_min, y_max = X_mat[:, 1].min() - 1, X_mat[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_step_size), np.arange(y_min, y_max, mesh_step_size)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) # Put the result into a color plot Z = Z.reshape(xx.shape) plt.figure() plt.pcolormesh(xx, yy, Z, cmap=cmap_light) # Plot training points plt.scatter(X_mat[:, 0], X_mat[:, 1], s=plot_symbol_size, c=y, cmap=cmap_bold, edgecolor = 'black') plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) patch0 = mpatches.Patch(color='#FF0000', label='apple') patch1 = mpatches.Patch(color='#00FF00', label='mandarin') patch2 = mpatches.Patch(color='#0000FF', label='orange') patch3 = mpatches.Patch(color='#AFAFAF', label='lemon') plt.legend(handles=[patch0, patch1, patch2, patch3]) plt.xlabel('height (cm)') plt.ylabel('width (cm)') plt.show() plot_fruit_knn(X_train, y_train, 5, 'uniform') # n_neighbors = 5","title":"Kesepuluh"},{"location":"#kesebelas","text":"Melakukan prediksi terhadap beberapa data baru. fruit_prediction = knn.predict([[30, 6, 5]]) lookup_fruit_name[fruit_prediction[0]] Output: mandarin fruit_prediction = knn.predict([[500, 500, 500]]) lookup_fruit_name[fruit_prediction[0]] Output: orange Selanjutnya ulangi langkah kesebelas untuk melakukan test data baru untuk diuji dengan K-Nearest Neighbor Classification.","title":"Kesebelas"},{"location":"#refrensi","text":"Gorunescu, F. 2011. Data Mining Concept Model and Techniques. Berlin: Springer. ISBN 978-3-642-19720-8. Han, Jiawei dan Kamber, Micheline. (2006), Data Mining : Concept and Techniques Second Edition, Morgan Kaufmann Publishers. Florin Gorunescu, Data Mining: Concepts, Models and Techniques, Springer, 2011. https://cahyadsn.phpindonesia.id/extra/knn.php https://www.academia.edu/31306621/MAKALAH_KNN_K-NEAREST_NEIGHBOUR_","title":"Refrensi:"},{"location":"getting-started/","text":"K-Means Pendahuluan Pengertian \u200b Data Clustering merupakan salah satu metode Data Mining yang bersifat tanpa arahan (unsupervised). Ada dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu hierarchical (hirarki) data clustering dan non-hierarchical (non hirarki) data clustering. K-Means merupakan salah satu metode data clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster/ kelompok. \u200b Metode ini mempartisi data ke dalam cluster/ kelompok sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karakteristik yang berbeda dikelompokkan ke dalam kelompok yang lain. Adapun tujuan dari data clustering ini adalah untuk meminimalisasikan objective function yang diset dalam proses clustering, yang pada umumnya berusaha meminimalisasikan variasi di dalam suatu cluster dan memaksimalisasikan variasi antar cluster. Manfaat Clustering adalah sebagai Identifikasi Object (Recognition) misalnya dalam bidang Image Processing, Computer Vision atau robot vision. Selain itu adalah sebagai Sistem Pendukung Keputusan dan Data Mining seperti Segmentasi pasar, pemetaan wilayah, Manajemen marketing dll. K-means clustering merupakan salah satu metode data clustering non-hirarki yang mengelompokan data dalam bentuk satu atau lebih cluster/kelompok. Data-data yang memiliki karakteristik yang sama dikelompokan dalam satu cluster/kelompok dan data yang memiliki karakteristik yang berbeda dikelompokan dengan cluster/kelompok yang lain sehingga data yang berada dalam satu cluster/kelompok memiliki tingkat variasi yang kecil (Agusta, 2007). Karakteristik K-Mean K-means sangat cepat dalam proses clustering. K-means sangat sensitive pada pembangkitan centroid awal secara random. Memungkinkan suatu cluster tidak mempunyai anggota. Hasil clustering dengan K-means bersifat unik (selalu berubah-ubah, terkadang baik, terkadang jelek). Tujuan Analisis Cluster Untuk mengelompokkan objek-objek (individu-individu) menjadi kelompok-kelompok yang mempunyai sifat yang relatif sama (homogen). Untuk membedakan dengan jelas antara satu kelompok ( cluster ) dengan kelompok lainnya. Manfaat Analisis Cluster Untuk menerapkan dasar-dasar pengelompokan dengan lebih konsisten. Untuk mengembangkan suatu metode generalisasi secara induktif, yaitu pengambilan kesimpulan secara umum dengan berdasarkan fakta-fakta khusus. Menemukan tipologi yang cocok dengan karakter obyek yang diteliti. Mendiskripsikan sifat-sifat/karakteristik dari masing-masing kelompok. Algoritma \u200b Data clustering menggunakan metode K-Means ini secara umum dilakukan dengan algoritma dasar sebagai berikut (Yudi Agusta, 2007) : Tentukan jumlah cluster. Alokasikan data ke dalam cluster secara random. Hitung centroid/ rata-rata dari data yang ada di masing-masing cluster. Alokasikan masing-masing data ke centroid/ rata-rata terdekat. Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan. Eucledian Distance \u200b Untuk mengelompokkan sebuah data pada kelompok tertentu, hal yang harus dilakukan adalah menghitung jarak antara data dengan centroid. Beberapa distance space telah diimplementasikan dalam menghitung jarak ( distance antara data dan centroid ) termasuk di antaranya L1 ( Manhattan/ City Block distance space , L2 ( Euclidean ) distance space , dan Lp ( Minkowski ) distance space . Jarak antara dua titik x1 dan x2 pada Manhattan/City Block distance space dihitung dengan menggunakan rumus sebagai berikut (Yudi Agusta, 2007): (1) (2) Dimana: DL2 : jarak kuadrat Euclidean antar onjek ke x2 dengan x1 P : jumlah variabel cluster x2j : nilai atau data dari objek ke-2 pada variabel ke-j x1j : nilai atau data dari obbjek ke-1 ada variabel ke-j (Everitt, 1993). Kelebihan Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan pembelajaran ini relatif cepat. Mudah untuk diadaptasi. Umum digunakan. Kekurangan Sebelum algoritma dijalankan, k buah titik diinisialisasi secara random sehingga pengelompokkan data yang dihasilkan dapat berbeda-beda. Jika nilai random untuk inisialisasi kurang baik, maka pengelompokkan yang dihasilkan pun menjadi kurang optimal. Dapat terjebak dalam masalah yang disebut curse of dimensionality . Hal ini dapat terjadi jika data pelatihan memiliki dimensi yang sangat tinggi (Contoh jika data pelatihan terdiri dari 2 atribut maka dimensinya adalah 2 dimensi. Namun jika ada 20 atribut, maka akan ada 20 dimensi). Salah satu cara kerja algoritma ini adalah mencari jarak terdekat antara k buah titik dengan titik lainnya. Jika mencari jarak antar titik pada 2 dimensi, masih mudah dilakukan. Namun bagaimana mencari jarak antar titik jika terdapat 20 dimensi. Hal ini akan menjadi sulit. Jika hanya terdapat beberapa titik sampel data, maka cukup mudah untuk menghitung dan mencari titik terdekat dengan k titik yang diinisialisasi secara random . Namun jika terdapat banyak sekali titik data (misalnya satu milyar buah data), maka perhitungan dan pencarian titik terdekat akan membutuhkan waktu yang lama. Proses tersebut dapat dipercepat, namun dibutuhkan struktur data yang lebih rumit seperti kD-Tree atau hashing . Implementasi (Studi Kasus) menentukan data yang akan dianalisis menghitung centroid/ rata-rata dari data yang ada di masing-masing cluster. Alokasikan masing-masing data ke centroid/ rata-rata terdekat.Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan. iterasi 1 rata-rata iterasi 1 lanjutkan sampai datanya sama,lanjutkan ke iterasi 2 iterasi 2 rata-rata iterasi 2 karena data yang di iterasi masih berbeda maka dilakukan iterasi ketiga iterasi 3 rata-rata iterasi 3 lakukanlah iterasi sampai data tersebut sama iterasi ke 4 rata-rata iterasi ke 4 karena datanya sama setelah dilakukan iterasi ke 4 maka iterasi dihentikan Refrensi: Agusta, Y. 2007. K-means - Penerapan, Permasalahan dan Metode Terkait. Jurnal Sistem dan Informatika Vol. 3 (Februari 2007): 47-60. Santosa, B. 2007. Data Mining: Teknik Pemanfaatan Data untuk Keperluan Bisnis. Yogyakarta: Graha Ilmu. Dalam jurnal SNTIKI, vol (5), Hal 395-398, oleh Nengsih W, Febiyanto pada tahun 2012 dengan judul \u201cData Mining Analysis Pengelompokan Penerima Beasiswa Menggunakan Teknik Clustering K-Means. https://id.wikipedia.org/wiki/K-means https://www.codepolitan.com/5-library-python-untuk-data-science-59b774b6cad97","title":"K-Means"},{"location":"getting-started/#k-means","text":"","title":"K-Means"},{"location":"getting-started/#pendahuluan","text":"","title":"Pendahuluan"},{"location":"getting-started/#pengertian","text":"\u200b Data Clustering merupakan salah satu metode Data Mining yang bersifat tanpa arahan (unsupervised). Ada dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu hierarchical (hirarki) data clustering dan non-hierarchical (non hirarki) data clustering. K-Means merupakan salah satu metode data clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster/ kelompok. \u200b Metode ini mempartisi data ke dalam cluster/ kelompok sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karakteristik yang berbeda dikelompokkan ke dalam kelompok yang lain. Adapun tujuan dari data clustering ini adalah untuk meminimalisasikan objective function yang diset dalam proses clustering, yang pada umumnya berusaha meminimalisasikan variasi di dalam suatu cluster dan memaksimalisasikan variasi antar cluster. Manfaat Clustering adalah sebagai Identifikasi Object (Recognition) misalnya dalam bidang Image Processing, Computer Vision atau robot vision. Selain itu adalah sebagai Sistem Pendukung Keputusan dan Data Mining seperti Segmentasi pasar, pemetaan wilayah, Manajemen marketing dll. K-means clustering merupakan salah satu metode data clustering non-hirarki yang mengelompokan data dalam bentuk satu atau lebih cluster/kelompok. Data-data yang memiliki karakteristik yang sama dikelompokan dalam satu cluster/kelompok dan data yang memiliki karakteristik yang berbeda dikelompokan dengan cluster/kelompok yang lain sehingga data yang berada dalam satu cluster/kelompok memiliki tingkat variasi yang kecil (Agusta, 2007).","title":"Pengertian"},{"location":"getting-started/#karakteristik-k-mean","text":"K-means sangat cepat dalam proses clustering. K-means sangat sensitive pada pembangkitan centroid awal secara random. Memungkinkan suatu cluster tidak mempunyai anggota. Hasil clustering dengan K-means bersifat unik (selalu berubah-ubah, terkadang baik, terkadang jelek).","title":"Karakteristik K-Mean"},{"location":"getting-started/#tujuan-analisis-cluster","text":"Untuk mengelompokkan objek-objek (individu-individu) menjadi kelompok-kelompok yang mempunyai sifat yang relatif sama (homogen). Untuk membedakan dengan jelas antara satu kelompok ( cluster ) dengan kelompok lainnya.","title":"Tujuan Analisis Cluster"},{"location":"getting-started/#manfaat-analisis-cluster","text":"Untuk menerapkan dasar-dasar pengelompokan dengan lebih konsisten. Untuk mengembangkan suatu metode generalisasi secara induktif, yaitu pengambilan kesimpulan secara umum dengan berdasarkan fakta-fakta khusus. Menemukan tipologi yang cocok dengan karakter obyek yang diteliti. Mendiskripsikan sifat-sifat/karakteristik dari masing-masing kelompok.","title":"Manfaat Analisis Cluster"},{"location":"getting-started/#algoritma","text":"\u200b Data clustering menggunakan metode K-Means ini secara umum dilakukan dengan algoritma dasar sebagai berikut (Yudi Agusta, 2007) : Tentukan jumlah cluster. Alokasikan data ke dalam cluster secara random. Hitung centroid/ rata-rata dari data yang ada di masing-masing cluster. Alokasikan masing-masing data ke centroid/ rata-rata terdekat. Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan.","title":"Algoritma"},{"location":"getting-started/#eucledian-distance","text":"\u200b Untuk mengelompokkan sebuah data pada kelompok tertentu, hal yang harus dilakukan adalah menghitung jarak antara data dengan centroid. Beberapa distance space telah diimplementasikan dalam menghitung jarak ( distance antara data dan centroid ) termasuk di antaranya L1 ( Manhattan/ City Block distance space , L2 ( Euclidean ) distance space , dan Lp ( Minkowski ) distance space . Jarak antara dua titik x1 dan x2 pada Manhattan/City Block distance space dihitung dengan menggunakan rumus sebagai berikut (Yudi Agusta, 2007): (1) (2) Dimana: DL2 : jarak kuadrat Euclidean antar onjek ke x2 dengan x1 P : jumlah variabel cluster x2j : nilai atau data dari objek ke-2 pada variabel ke-j x1j : nilai atau data dari obbjek ke-1 ada variabel ke-j (Everitt, 1993).","title":"Eucledian Distance"},{"location":"getting-started/#kelebihan","text":"Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan pembelajaran ini relatif cepat. Mudah untuk diadaptasi. Umum digunakan.","title":"Kelebihan"},{"location":"getting-started/#kekurangan","text":"Sebelum algoritma dijalankan, k buah titik diinisialisasi secara random sehingga pengelompokkan data yang dihasilkan dapat berbeda-beda. Jika nilai random untuk inisialisasi kurang baik, maka pengelompokkan yang dihasilkan pun menjadi kurang optimal. Dapat terjebak dalam masalah yang disebut curse of dimensionality . Hal ini dapat terjadi jika data pelatihan memiliki dimensi yang sangat tinggi (Contoh jika data pelatihan terdiri dari 2 atribut maka dimensinya adalah 2 dimensi. Namun jika ada 20 atribut, maka akan ada 20 dimensi). Salah satu cara kerja algoritma ini adalah mencari jarak terdekat antara k buah titik dengan titik lainnya. Jika mencari jarak antar titik pada 2 dimensi, masih mudah dilakukan. Namun bagaimana mencari jarak antar titik jika terdapat 20 dimensi. Hal ini akan menjadi sulit. Jika hanya terdapat beberapa titik sampel data, maka cukup mudah untuk menghitung dan mencari titik terdekat dengan k titik yang diinisialisasi secara random . Namun jika terdapat banyak sekali titik data (misalnya satu milyar buah data), maka perhitungan dan pencarian titik terdekat akan membutuhkan waktu yang lama. Proses tersebut dapat dipercepat, namun dibutuhkan struktur data yang lebih rumit seperti kD-Tree atau hashing .","title":"Kekurangan"},{"location":"getting-started/#implementasi-studi-kasus","text":"menentukan data yang akan dianalisis menghitung centroid/ rata-rata dari data yang ada di masing-masing cluster. Alokasikan masing-masing data ke centroid/ rata-rata terdekat.Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan. iterasi 1 rata-rata iterasi 1 lanjutkan sampai datanya sama,lanjutkan ke iterasi 2 iterasi 2 rata-rata iterasi 2 karena data yang di iterasi masih berbeda maka dilakukan iterasi ketiga iterasi 3 rata-rata iterasi 3 lakukanlah iterasi sampai data tersebut sama iterasi ke 4 rata-rata iterasi ke 4 karena datanya sama setelah dilakukan iterasi ke 4 maka iterasi dihentikan","title":"Implementasi (Studi Kasus)"},{"location":"getting-started/#refrensi","text":"Agusta, Y. 2007. K-means - Penerapan, Permasalahan dan Metode Terkait. Jurnal Sistem dan Informatika Vol. 3 (Februari 2007): 47-60. Santosa, B. 2007. Data Mining: Teknik Pemanfaatan Data untuk Keperluan Bisnis. Yogyakarta: Graha Ilmu. Dalam jurnal SNTIKI, vol (5), Hal 395-398, oleh Nengsih W, Febiyanto pada tahun 2012 dengan judul \u201cData Mining Analysis Pengelompokan Penerima Beasiswa Menggunakan Teknik Clustering K-Means. https://id.wikipedia.org/wiki/K-means https://www.codepolitan.com/5-library-python-untuk-data-science-59b774b6cad97","title":"Refrensi:"},{"location":"tree/","text":"Decision Tree Pendahuluan Pengertian Decision Tree \u200b Decision Tree (Pohon Keputusan) adalah pohon dimana setiap cabangnyamenunjukkan pilihan diantara sejumlah alternatif pilihan yang ada, dan setiapdaunnya menunjukkan keputusan yang dipilih.Decision tree biasa digunakan untuk mendapatkan informasi untuk tujuanpengambilan sebuah keputusan. Decision tree dimulai dengan sebuah root node(titik awal) yang dipakai oleh user untuk mengambil tindakan. Dari node root ini,user memecahnya sesuai dengan algoritma decision tree. \u200b Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Nama lain dari decision tree adalah CART ( Classification and Regression Tree ). Dimana metode ini merupakan gabungan dari dua jenis pohon, yaitu classification tree dan juga regression tree . Decision tree dibentuk dari 3 tipe dari simpul: simpul root, simpul perantara, dan simpul leaf. Simpul leaf memuat suatu keputusan akhir atau kelas target untuk suatu pohon keputusan. Simpul root adalah tiitk awal dari suatu decision tree. Setiap simpul perantara berhubungan dengan suatu pertanyaan atau pengujian. Algoritma Pohon dibangun dalam suatu metoda rekursif topdown divide and-conquer. Seluruh contoh pelatihan dimulai dari simpul root, lalu dilakukan penujian. Mencabang ke jalur yang benar berdasarkan hasil pengujian. Apakah simpul leaf ditemukan? Jika true , masukkan ke kelas target, jika false kembali ke langkah awal. Atribut-atribut berada dalam suatu kategori (jika bernilai kontinu, nilai-nilai tersebut didistribusikan terlebih dahulu). Contoh-contoh dipartisi secara rekursif berdasarkan atribut terpilih. Atribut-atribut uji dipilih berdasarakn heuristik atau pengukurann statistik (misal, information gain ). Rumus Menghitung *Entrophy * S : Himpunan kasus k : Jumlah partisi S Pj : Probabilitas yang didapat dari jumlah (Ya/Tidak) dibagi total kasus Menghitung *Gain * S : Himpunan kasus A : Atribut n : jumlah partisi atribut A |Si| : jumlah kasus pada partisi ke-i |S| : jumlah kasus dalam S Contoh Kasus \u200b Disini setiap percabangan menyatakan kondisi yang harus dipenuhi dan tiap ujung pohon menyatakan kelas data. Contoh di Gambar 1 adalah identifikasi pembeli komputer,dari pohon keputusan tersebut diketahui bahwa salah satu kelompok yang potensial membeli komputer adalah orang yang berusia di bawah 30 tahun dan juga pelajar. Setelah sebuah pohon keputusan dibangun maka dapat digunakan untuk mengklasifikasikan record yang belum ada kelasnya. Dimulai dari node root , menggunakan tes terhadap atribut dari record yang belum ada kelasnya tersebut lalu mengikuti cabang yang sesuai dengan hasil dari tes tersebut, yang akan membawa kepada internal node ( node yang memiliki satu cabang masuk dan dua atau lebih cabang yang keluar), dengan cara harus melakukan tes lagi terhadap atribut atau node daun. Record yang kelasnya tidak diketahui kemudian diberikan kelas yang sesuai dengan kelas yang ada pada node daun. Pada pohon keputusan setiap simpul daun menandai label kelas. Proses dalam pohon keputusan yaitu mengubah bentuk data (tabel) menjadi model pohon ( tree ) kemudian mengubah model pohon tersebut menjadi aturan ( rule ). Kelebihan Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi lebih simpel dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode decision tree maka sample diuji hanya berdasarkan kriteria atau kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Kefleksibelan metode decision tree ini meningkatkan kualitas keputusan yang dihasilkan jika dibandingkan ketika menggunakan metode penghitungan satu tahap yang lebih konvensional. Dalam analisis multivariat, dengan kriteria dan kelas yang jumlahnya sangat banyak, seorang penguji biasanya perlu untuk mengestimasikan baik itu distribusi dimensi tinggi ataupun parameter tertentu dari distribusi kelas tersebut. Metode decision tree dapat menghindari munculnya permasalahan ini dengan menggunakan criteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan. Kekurangan Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah decision tree yang besar. Kesulitan dalam mendesain decision tree yang optimal. Hasil kualitas keputusan yang didapatkan dari metode decision tree sangat tergantung pada bagaimana pohon tersebut didesain. Implementasi (Studi Kasus) Alat & Bahan Sebelum menerapkan konsep decision tree pada studi kasus yang telah ditentukan, beberapa tools yang perlu dipersiapkan agar program yang kita rancang bisa dieksekusi dengan baik diantaranya: python 3.x (versi 3 keatas). Anaconda Navigator atau Pycharm. Untuk mempermudah kawan-kawan mendapatkan toolsnya, sillakan kawan-kawan bisa download tools-nya disini . Studi kasus pada Balance Scales adalah mengklasifikasikan beberapa poin perolehan suatu percobaan dengan 3 klasifikasi objek. Untuk contoh penulisan program dan datanya bisa mengambil kawan-kawan download disini . Berikut source code dan penjelasan untuk menyelesaikan study kasus tersebut dengan konsep Decision Tree Clasification. Langkah-langkah: Pertama Import beberapa library dari python seperti: pandas => memuat sebuah file ke dalam tabel virtual ala spreadsheet yang memiliki struktur data yang diperukan untuk membersihkan data mentah ke dalam sebuah bentuk yang cocok untuk dianalisis. numpy => untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. sklearn => untuk mengimportkan library data science . Berbagai fungsi didalamnya seperti fungsi agregasi, hitung metriks, hitung akurasi, display gambar, dan lain sebagainya. seaborn => library untuk membuat grafik statistik. pydotplus => library untuk memvisualisasikan bentuk hirarki. #import library import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn import metrics from sklearn.metrics import accuracy_score import seaborn as sns from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image from sklearn.tree import export_graphviz import pydotplus import numpy as np Kedua Mengimport data dari komputer dengan perintah pandas. #read data data = pd.read_csv('balance_scale.csv') #Pastikan file data set berada dalam folder yang sama dengan file jupyter notebook Ketiga Menampilkan data. #explore data data.head() Keempat Melihat info kolom dari data. data.info() Kelima Memilih kolom uji untuk dihitung hasilnya. zero_not_accepted = ['berat_kiri','jarak_kiri','berat_kanan','jarak_kanan'] # for col in zero_not_accepted: # for i in data[col]: # if i==0: # colSum = sum(data[col]) # meanCol=colSum/len(data[col]) # data[col]=meanCol for col in zero_not_accepted: data[col]= data[col].replace(0,np.NaN) mean = int(data[col].mean(skipna=True)) data[col] = data[col].replace(np.NaN,mean) Keenam Membagi data train dan data test dengan data test 30%. X = data.iloc[:,0:3] #memilih objek data X dengan array y = data.iloc[:,3] #memilih objek data y dengan array #build model & train data X = data[['berat_kiri','jarak_kiri','berat_kanan','jarak_kanan']] #objek uji y = data['seimbang'] #objek kelas #split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0) Ketujuh Menentukan entropy data. clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4) #entropy dengan 4 cabang clf = clf.fit(X_train,y_train) y_pred = clf.predict(X_test) Kedelapan Meenentukan simpul root, simpul perantara, dan simpul leaf dari data yang telah diketahui nilai entropy-nya. feature_cols = ['berat_kiri','jarak_kiri','berat_kanan','jarak_kanan'] #kolom yang diuji #mengelompokkan data ke dalam kelas dot_data = StringIO() export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=['B','R','L']) #visualisai pohon keputusan graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) #menyimpan hasil visualisasi graph.write_png('keseimbangan.png') Image(graph.create_png()) Referensi Discovering Knowledge in Data (Introduction to Data Mining), Chapter 6, Daniel T. Larose, Wiley, 2004. Marwana. Algoritma C4.5 Untuk Simulasi Prediksi Kemenangan Dalam Pertandingan Sepakbola. Jurnal Informatika Multimedia, STIMED NUSA PALAPA. http://tessy.lecturer.pens.ac.id/kuliah/db2/klasifikasi.pdf . https://en.wikipedia.org/wiki/Decision_tree .","title":"Decision-Tree"},{"location":"tree/#decision-tree","text":"","title":"Decision Tree"},{"location":"tree/#pendahuluan","text":"","title":"Pendahuluan"},{"location":"tree/#pengertian-decision-tree","text":"\u200b Decision Tree (Pohon Keputusan) adalah pohon dimana setiap cabangnyamenunjukkan pilihan diantara sejumlah alternatif pilihan yang ada, dan setiapdaunnya menunjukkan keputusan yang dipilih.Decision tree biasa digunakan untuk mendapatkan informasi untuk tujuanpengambilan sebuah keputusan. Decision tree dimulai dengan sebuah root node(titik awal) yang dipakai oleh user untuk mengambil tindakan. Dari node root ini,user memecahnya sesuai dengan algoritma decision tree. \u200b Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Nama lain dari decision tree adalah CART ( Classification and Regression Tree ). Dimana metode ini merupakan gabungan dari dua jenis pohon, yaitu classification tree dan juga regression tree . Decision tree dibentuk dari 3 tipe dari simpul: simpul root, simpul perantara, dan simpul leaf. Simpul leaf memuat suatu keputusan akhir atau kelas target untuk suatu pohon keputusan. Simpul root adalah tiitk awal dari suatu decision tree. Setiap simpul perantara berhubungan dengan suatu pertanyaan atau pengujian.","title":"Pengertian Decision Tree"},{"location":"tree/#algoritma","text":"Pohon dibangun dalam suatu metoda rekursif topdown divide and-conquer. Seluruh contoh pelatihan dimulai dari simpul root, lalu dilakukan penujian. Mencabang ke jalur yang benar berdasarkan hasil pengujian. Apakah simpul leaf ditemukan? Jika true , masukkan ke kelas target, jika false kembali ke langkah awal. Atribut-atribut berada dalam suatu kategori (jika bernilai kontinu, nilai-nilai tersebut didistribusikan terlebih dahulu). Contoh-contoh dipartisi secara rekursif berdasarkan atribut terpilih. Atribut-atribut uji dipilih berdasarakn heuristik atau pengukurann statistik (misal, information gain ).","title":"Algoritma"},{"location":"tree/#rumus","text":"Menghitung *Entrophy * S : Himpunan kasus k : Jumlah partisi S Pj : Probabilitas yang didapat dari jumlah (Ya/Tidak) dibagi total kasus Menghitung *Gain * S : Himpunan kasus A : Atribut n : jumlah partisi atribut A |Si| : jumlah kasus pada partisi ke-i |S| : jumlah kasus dalam S Contoh Kasus \u200b Disini setiap percabangan menyatakan kondisi yang harus dipenuhi dan tiap ujung pohon menyatakan kelas data. Contoh di Gambar 1 adalah identifikasi pembeli komputer,dari pohon keputusan tersebut diketahui bahwa salah satu kelompok yang potensial membeli komputer adalah orang yang berusia di bawah 30 tahun dan juga pelajar. Setelah sebuah pohon keputusan dibangun maka dapat digunakan untuk mengklasifikasikan record yang belum ada kelasnya. Dimulai dari node root , menggunakan tes terhadap atribut dari record yang belum ada kelasnya tersebut lalu mengikuti cabang yang sesuai dengan hasil dari tes tersebut, yang akan membawa kepada internal node ( node yang memiliki satu cabang masuk dan dua atau lebih cabang yang keluar), dengan cara harus melakukan tes lagi terhadap atribut atau node daun. Record yang kelasnya tidak diketahui kemudian diberikan kelas yang sesuai dengan kelas yang ada pada node daun. Pada pohon keputusan setiap simpul daun menandai label kelas. Proses dalam pohon keputusan yaitu mengubah bentuk data (tabel) menjadi model pohon ( tree ) kemudian mengubah model pohon tersebut menjadi aturan ( rule ).","title":"Rumus"},{"location":"tree/#kelebihan","text":"Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi lebih simpel dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode decision tree maka sample diuji hanya berdasarkan kriteria atau kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Kefleksibelan metode decision tree ini meningkatkan kualitas keputusan yang dihasilkan jika dibandingkan ketika menggunakan metode penghitungan satu tahap yang lebih konvensional. Dalam analisis multivariat, dengan kriteria dan kelas yang jumlahnya sangat banyak, seorang penguji biasanya perlu untuk mengestimasikan baik itu distribusi dimensi tinggi ataupun parameter tertentu dari distribusi kelas tersebut. Metode decision tree dapat menghindari munculnya permasalahan ini dengan menggunakan criteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan.","title":"Kelebihan"},{"location":"tree/#kekurangan","text":"Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah decision tree yang besar. Kesulitan dalam mendesain decision tree yang optimal. Hasil kualitas keputusan yang didapatkan dari metode decision tree sangat tergantung pada bagaimana pohon tersebut didesain.","title":"Kekurangan"},{"location":"tree/#implementasi-studi-kasus","text":"","title":"Implementasi (Studi Kasus)"},{"location":"tree/#alat-bahan","text":"Sebelum menerapkan konsep decision tree pada studi kasus yang telah ditentukan, beberapa tools yang perlu dipersiapkan agar program yang kita rancang bisa dieksekusi dengan baik diantaranya: python 3.x (versi 3 keatas). Anaconda Navigator atau Pycharm. Untuk mempermudah kawan-kawan mendapatkan toolsnya, sillakan kawan-kawan bisa download tools-nya disini . Studi kasus pada Balance Scales adalah mengklasifikasikan beberapa poin perolehan suatu percobaan dengan 3 klasifikasi objek. Untuk contoh penulisan program dan datanya bisa mengambil kawan-kawan download disini . Berikut source code dan penjelasan untuk menyelesaikan study kasus tersebut dengan konsep Decision Tree Clasification.","title":"Alat &amp; Bahan"},{"location":"tree/#langkah-langkah","text":"","title":"Langkah-langkah:"},{"location":"tree/#pertama","text":"Import beberapa library dari python seperti: pandas => memuat sebuah file ke dalam tabel virtual ala spreadsheet yang memiliki struktur data yang diperukan untuk membersihkan data mentah ke dalam sebuah bentuk yang cocok untuk dianalisis. numpy => untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. sklearn => untuk mengimportkan library data science . Berbagai fungsi didalamnya seperti fungsi agregasi, hitung metriks, hitung akurasi, display gambar, dan lain sebagainya. seaborn => library untuk membuat grafik statistik. pydotplus => library untuk memvisualisasikan bentuk hirarki. #import library import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn import metrics from sklearn.metrics import accuracy_score import seaborn as sns from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image from sklearn.tree import export_graphviz import pydotplus import numpy as np","title":"Pertama"},{"location":"tree/#kedua","text":"Mengimport data dari komputer dengan perintah pandas. #read data data = pd.read_csv('balance_scale.csv') #Pastikan file data set berada dalam folder yang sama dengan file jupyter notebook","title":"Kedua"},{"location":"tree/#ketiga","text":"Menampilkan data. #explore data data.head()","title":"Ketiga"},{"location":"tree/#keempat","text":"Melihat info kolom dari data. data.info()","title":"Keempat"},{"location":"tree/#kelima","text":"Memilih kolom uji untuk dihitung hasilnya. zero_not_accepted = ['berat_kiri','jarak_kiri','berat_kanan','jarak_kanan'] # for col in zero_not_accepted: # for i in data[col]: # if i==0: # colSum = sum(data[col]) # meanCol=colSum/len(data[col]) # data[col]=meanCol for col in zero_not_accepted: data[col]= data[col].replace(0,np.NaN) mean = int(data[col].mean(skipna=True)) data[col] = data[col].replace(np.NaN,mean)","title":"Kelima"},{"location":"tree/#keenam","text":"Membagi data train dan data test dengan data test 30%. X = data.iloc[:,0:3] #memilih objek data X dengan array y = data.iloc[:,3] #memilih objek data y dengan array #build model & train data X = data[['berat_kiri','jarak_kiri','berat_kanan','jarak_kanan']] #objek uji y = data['seimbang'] #objek kelas #split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)","title":"Keenam"},{"location":"tree/#ketujuh","text":"Menentukan entropy data. clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4) #entropy dengan 4 cabang clf = clf.fit(X_train,y_train) y_pred = clf.predict(X_test)","title":"Ketujuh"},{"location":"tree/#kedelapan","text":"Meenentukan simpul root, simpul perantara, dan simpul leaf dari data yang telah diketahui nilai entropy-nya. feature_cols = ['berat_kiri','jarak_kiri','berat_kanan','jarak_kanan'] #kolom yang diuji #mengelompokkan data ke dalam kelas dot_data = StringIO() export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=['B','R','L']) #visualisai pohon keputusan graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) #menyimpan hasil visualisasi graph.write_png('keseimbangan.png') Image(graph.create_png())","title":"Kedelapan"},{"location":"tree/#referensi","text":"Discovering Knowledge in Data (Introduction to Data Mining), Chapter 6, Daniel T. Larose, Wiley, 2004. Marwana. Algoritma C4.5 Untuk Simulasi Prediksi Kemenangan Dalam Pertandingan Sepakbola. Jurnal Informatika Multimedia, STIMED NUSA PALAPA. http://tessy.lecturer.pens.ac.id/kuliah/db2/klasifikasi.pdf . https://en.wikipedia.org/wiki/Decision_tree .","title":"Referensi"}]}